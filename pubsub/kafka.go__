//package pubsub
//
//import (
//	"context"
//	"encoding/json"
//	"fmt"
//	"github.com/IBM/sarama"
//	"log"
//	"strconv"
//	"strings"
//	"time"
//)
//
//type KafkaPubSubClient struct {
//	producer       sarama.SyncProducer
//	configMap      *sarama.Config
//	configConsumer sarama.ConsumerGroup
//	topicList      map[string]bool
//	// topicConsumeList []string
//	topicQueueList map[string]bool
//	connStringList map[string]string
//	connString     string
//}
//
//var topicStr = ""
//
//func (o *KafkaPubSubClient) Publish(subject string, msg interface{}) error {
//
//	var msgByte []byte
//	var err error
//
//	switch msg.(type) {
//	case int, int16, int32, int64, string, float32, float64, bool:
//		msgString := fmt.Sprintf("%v", msg)
//		msgByte = []byte(msgString)
//	default:
//		msgByte, err = json.Marshal(msg)
//
//		if err != nil {
//			return err
//		}
//	}
//	topic := subject
//	log.Printf("msgByte is %s", msgByte)
//	msgKafka := &sarama.ProducerMessage{
//		Topic: topic,
//		Value: sarama.StringEncoder(msgByte),
//	}
//
//	partition, offset, err := o.producer.SendMessage(msgKafka)
//	if err != nil {
//		log.Printf("Failed to produce message: %v", err)
//	} else {
//		log.Printf("Produced message to topic %s, partition %d, offset %d\n", topic, partition, offset)
//	}
//
//	return err
//}
//
//func (o *KafkaPubSubClient) Request(subject string, msg interface{}, timeOut ...time.Duration) (string, error) {
//	return "", nil
//}
//
//func (o *KafkaPubSubClient) Subscribe(subject string, eventHandler PubSubEventHandler) {
//	// Create a Kafka consumer instance
//	fmt.Printf("Prepare consuming %v topic\n", subject)
//	fmt.Printf("Prepare coba is %v\n", topicStr)
//	topicStrArr := strings.Split(topicStr, "__")
//	fmt.Printf("check topicStr %v is running %v \n", subject, topicStr)
//	if !o.includes(topicStrArr, subject) {
//		fmt.Printf("Start consuming %v topic\n", subject)
//		if topicStr == "" {
//			topicStr = subject
//		} else {
//			topicStr = topicStr + "__" + subject
//		}
//		fmt.Printf("topicStr is %v\n", topicStr)
//		// *topicConsumeList = append(*topicConsumeList, subject)
//		o.createKafkaTopic(subject) // check if kafka start before producer
//		// ctx, _ := context.WithCancel(context.Background())
//		brokers := []string{}
//		delimiter3 := ","
//		substrings3 := strings.Split(o.connStringList["bootstrap.servers"], delimiter3)
//		for _, substring3 := range substrings3 {
//			brokers = append(brokers, substring3)
//		}
//
//		config := sarama.NewConfig()
//		config.Consumer.Return.Errors = true
//		configConsumer, err := sarama.NewConsumerGroup(brokers, "my-group-"+subject, config)
//		if err != nil {
//			fmt.Printf("Failed to start Sarama consumer: %v\n", err)
//		}
//		go o.consumeFromGroup(configConsumer, subject, eventHandler)
//	}
//}
//
//func (o *KafkaPubSubClient) RequestSubscribe(subject string, eventHandler PubSubReqEventHandler) {
//}
//
//func (o *KafkaPubSubClient) QueueSubscribe(subject string, queue string, eventHandler PubSubEventHandler) {
//	fmt.Printf("Prepare consuming queue %v topic %v queue\n", subject, queue)
//	fmt.Printf("topicQueueList %v \n", o.topicQueueList)
//	if !o.topicQueueList[subject+"_"+queue] {
//		log.Printf("start consuming %v topic and %v queue ", subject, queue)
//		o.createKafkaTopic(subject) // check if kafka start before producer
//		brokers := []string{}
//		delimiter3 := ","
//		substrings3 := strings.Split(o.connStringList["bootstrap.servers"], delimiter3)
//		for _, substring3 := range substrings3 {
//			brokers = append(brokers, substring3)
//		}
//
//		config := sarama.NewConfig()
//		config.Consumer.Group.Rebalance.Strategy = sarama.BalanceStrategySticky
//		config.Consumer.Return.Errors = true
//		queueConsumer, err := sarama.NewConsumerGroup(brokers, queue, config)
//		if err != nil {
//			fmt.Printf("Error creating queue Kafka consumer: %v\n", err)
//			return
//		}
//		go o.consumeFromGroup(queueConsumer, subject, eventHandler)
//		o.topicQueueList[subject+"_"+queue] = true
//	}
//}
//
//func (o *KafkaPubSubClient) consumeFromGroup(consumer sarama.ConsumerGroup, topic string, eventHandler PubSubEventHandler) {
//	errCount := 0
//	for {
//		if errCount > 10 {
//			break
//		}
//		handler := &ConsumerHandler{
//			EventHandler: eventHandler,
//		}
//
//		log.Printf("debug start consuming name %v topic", topic)
//		if err := consumer.Consume(context.Background(), []string{topic}, handler); err != nil {
//			log.Printf("Error from consumer group: %v\n", err)
//			errCount++
//		}
//	}
//}
//
//func (o *KafkaPubSubClient) createKafkaTopic(topic string) {
//	// Create a Sarama configuration
//	// config := sarama.NewConfig()
//	o.configMap.Version = sarama.V2_8_0_0 // Specify the Kafka protocol version
//
//	// Create a Kafka Admin client
//	delimiter3 := ","
//	brokers := []string{} // Replace with your Kafka broker addresses
//	substrings3 := strings.Split(o.connStringList["bootstrap.servers"], delimiter3)
//	for _, substring3 := range substrings3 {
//		brokers = append(brokers, substring3)
//	}
//	adminClient, err := sarama.NewClusterAdmin(brokers, o.configMap)
//	if err != nil {
//		log.Fatalf("Error creating Kafka Admin client: %v", err)
//	}
//
//	topics, err := adminClient.ListTopics()
//	if err != nil {
//		log.Fatalf("Error getting topic list: %v", err)
//	}
//
//	// Print the list of topics
//	log.Println("List of Kafka topics:")
//	for topicSearch := range topics {
//		if topicSearch == topic {
//			log.Println("topic already exists")
//			return
//		}
//		// log.Println(topic)
//	}
//
//	// Define the topic configuration
//	replicate := "2"
//	if o.connStringList["replicas"] != "" {
//		replicate = o.connStringList["replicas"]
//	}
//	replicateInt, _ := strconv.Atoi(replicate)
//	replicasPointer := &replicate
//	topicConfig := &sarama.TopicDetail{
//		NumPartitions:     10,                  // Number of partitions for the topic
//		ReplicationFactor: int16(replicateInt), // Replication factor for the topic
//		ConfigEntries: map[string]*string{
//			"min.insync.replicas": replicasPointer, // Set based on your requirements
//		},
//	}
//
//	// Specify the topic name
//	topicName := topic
//
//	// Create the Kafka topic
//	err = adminClient.CreateTopic(topicName, topicConfig, false)
//	if err != nil {
//		log.Printf("Error creating Kafka topic: %v", err)
//	}
//	log.Printf("Kafka topic '%s' created successfully", topicName)
//}
//
//func (o *KafkaPubSubClient) checkTopic(topic string) {
//	// Check if the topic exists
//	if o.topicList[topic] {
//		fmt.Printf("Topic '%s' exists.\n", topic)
//	} else {
//		fmt.Printf("Topic '%s' does not exist.\n", topic)
//		o.createKafkaTopic(topic)
//	}
//}
//
//func (o *KafkaPubSubClient) consumeTopic(topic string, eventHandler PubSubEventHandler) {
//
//}
//
//func (o *KafkaPubSubClient) subscribeTopic(topic string, eventHandler PubSubEventHandler) {
//	ctx, _ := context.WithCancel(context.Background())
//
//	go func() {
//		for {
//			select {
//			case err := <-o.configConsumer.Errors():
//				log.Println("Error:", err)
//			case <-ctx.Done():
//				return
//			default:
//				if err := o.configConsumer.Consume(ctx, []string{topic}, newConsumerHandler(eventHandler)); err != nil {
//					log.Println("Error:", err)
//				}
//			}
//		}
//	}()
//}
//
//type ConsumerHandler struct {
//	EventHandler PubSubEventHandler
//}
//
//func newConsumerHandler(eventHandler PubSubEventHandler) sarama.ConsumerGroupHandler {
//	return &ConsumerHandler{
//		EventHandler: eventHandler,
//	}
//}
//
//func (h *ConsumerHandler) Setup(_ sarama.ConsumerGroupSession) error {
//	return nil
//}
//
//func (h *ConsumerHandler) Cleanup(_ sarama.ConsumerGroupSession) error {
//	return nil
//}
//
//func (h *ConsumerHandler) ConsumeClaim(session sarama.ConsumerGroupSession, claim sarama.ConsumerGroupClaim) error {
//	for message := range claim.Messages() {
//		fmt.Printf("Consumed message from partition %d with offset %d and topic %v: %s\n", message.Partition, message.Offset, message.Topic, string(message.Value))
//		h.EventHandler(message.Topic, string(message.Value))
//		session.MarkMessage(message, "")
//	}
//
//	return nil
//}
//
//func (o *KafkaPubSubClient) includes(arr []string, target string) bool {
//	for _, value := range arr {
//		if value == target {
//			return true
//		}
//	}
//	return false
//}
//
//func init() {
//	RegPubSubCreator("kafka", func(connString string) (PubSubClient, error) {
//		ret := &KafkaPubSubClient{}
//
//		var err error
//
//		// add tagging
//		inputString := connString
//		delimiter := ";"
//		delimiter2 := "="
//		delimiter3 := ","
//		mappingString := make(map[string]string)
//		ret.connStringList = mappingString
//
//		// Use strings.Split to split the string
//		substrings := strings.Split(inputString, delimiter)
//
//		// Print the resulting substrings
//		for _, substring := range substrings {
//			fmt.Println(substring)
//			substrings2 := strings.Split(substring, delimiter2)
//			ret.connStringList[substrings2[0]] = substrings2[1]
//		}
//
//		ret.connString = ret.connStringList["bootstrap.servers"]
//		brokers := []string{} // Replace with your Kafka broker addresses
//		substrings3 := strings.Split(ret.connStringList["bootstrap.servers"], delimiter3)
//		for i, substring3 := range substrings3 {
//			fmt.Println("server is " + substring3)
//			brokers = append(brokers, substring3)
//			fmt.Println("server loop is " + strconv.Itoa(i))
//		}
//
//		fmt.Printf("get brocker list complete")
//
//		config := sarama.NewConfig()
//		config.Producer.RequiredAcks = sarama.WaitForAll
//		config.Producer.Retry.Max = 5
//		config.Producer.Retry.Backoff = 1000 * time.Millisecond
//		// config.Producer.Flush.Frequency = 500 * time.Millisecond // Flush batches every 500ms
//		config.Producer.Return.Successes = true
//		config.Admin.Timeout = 5000 * time.Millisecond
//
//		ret.producer, err = sarama.NewSyncProducer(brokers, config)
//		if err != nil {
//			// log.Fatalf("Failed to start Sarama producer: %v", err)
//			fmt.Printf("Failed to start Sarama producer: %v\n", err)
//			return nil, err
//		}
//
//		config.Consumer.Return.Errors = true
//		// plan to delete this code
//		ret.configConsumer, err = sarama.NewConsumerGroup(brokers, "my-group", config)
//		if err != nil {
//			fmt.Printf("Failed to start Sarama consumer: %v\n", err)
//			return nil, err
//		}
//
//		ret.configMap = config
//
//		mapping := make(map[string]bool)
//		if ret.topicList == nil {
//			ret.topicList = mapping
//		}
//		if ret.topicQueueList == nil {
//			ret.topicQueueList = mapping
//		}
//
//		return ret, nil
//	})
//}
